{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PERSPECTIVES = [\"INFORMATION\", \"SUGGESTION\", \"EXPERIENCE\", \"QUESTION\", \"CAUSE\"]\n",
    "\n",
    "bleu = load(\"sacrebleu\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Tokenizer and base model\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "base_model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Perspective-aware attention layer\n",
    "class PerspectiveFusionAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, decoder_hidden, perspective_embed):\n",
    "        perspective_proj = perspective_embed.unsqueeze(1).expand(-1, decoder_hidden.size(1), -1)\n",
    "        q = self.query_proj(decoder_hidden)\n",
    "        k = self.key_proj(perspective_proj)\n",
    "        v = self.value_proj(perspective_proj)\n",
    "        out, _ = self.multihead_attn(q, k, v)\n",
    "        return out + decoder_hidden\n",
    "\n",
    "class CustomDecoderWithPerspective(nn.Module):\n",
    "    def __init__(self, base_model, num_perspectives):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.perspective_embed = nn.Embedding(num_perspectives, base_model.config.d_model)\n",
    "        self.perspective_fusion = PerspectiveFusionAttention(base_model.config.d_model,\n",
    "                                                              base_model.config.decoder_attention_heads)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, perspective_id, labels=None):\n",
    "        perspective_embed = self.perspective_embed(perspective_id)\n",
    "        decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels) if labels is not None else None\n",
    "\n",
    "        outputs = self.model.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            use_cache=False,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        decoder_hidden = outputs.decoder_hidden_states[-1]\n",
    "        fused = self.perspective_fusion(decoder_hidden, perspective_embed)\n",
    "        lm_logits = self.model.lm_head(fused)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(lm_logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": lm_logits}\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=4, lora_alpha=16, lora_dropout=0.1)\n",
    "base_model = get_peft_model(base_model, peft_config)\n",
    "model = CustomDecoderWithPerspective(base_model, num_perspectives=len(PERSPECTIVES)).to(device)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"./flanT5_files/train.json\",\n",
    "    \"val\": \"./flanT5_files/valid.json\"\n",
    "})\n",
    "\n",
    "# Function to clean summary text\n",
    "def clean_summary(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip().lower()\n",
    "    return \"\" if text in [\"false\", \"true\", \"n/a\", \"not_duplicate\", \"duplicate\", \"\"] else text\n",
    "\n",
    "def format_example(example):\n",
    "    context = f\"Context: {example.get('context', '').strip()}\\n\"\n",
    "    question = f\"Question: {example.get('question', '').strip()}\\n\"\n",
    "    answers = f\"Answers: {' '.join(example.get('answers', [])).strip()}\"\n",
    "    input_text = f\"{context}{question}{answers}\"\n",
    "    outputs = []\n",
    "    for p in PERSPECTIVES:\n",
    "        summary = clean_summary(example.get(\"labelled_summaries\", {}).get(f\"{p}_SUMMARY\", \"\"))\n",
    "        if summary:\n",
    "            outputs.append({\"input\": input_text, \"output\": summary, \"perspective\": p})\n",
    "    return outputs\n",
    "\n",
    "def trim(data): \n",
    "    return data.select(range(len(data)))\n",
    "\n",
    "train_data = sum([format_example(e) for e in trim(dataset[\"train\"])], [])\n",
    "val_data   = sum([format_example(e) for e in trim(dataset[\"val\"])], [])\n",
    "\n",
    "train_ds = Dataset.from_list(train_data)\n",
    "val_ds = Dataset.from_list(val_data)\n",
    "\n",
    "# Perspective mapping\n",
    "perspective2id = {p: i for i, p in enumerate(PERSPECTIVES)}\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(batch[\"output\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = [[(t if t != tokenizer.pad_token_id else -100) for t in l] for l in targets[\"input_ids\"]]\n",
    "    model_inputs[\"perspective_id\"] = [perspective2id[p] for p in batch[\"perspective\"]]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True)\n",
    "val_tok   = val_ds.map(preprocess, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./novel_generator\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    save_strategy=\"no\",        \n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "save_dir = \"novel_generator\"\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "model.model.save_pretrained(save_dir)\n",
    "\n",
    "if hasattr(model.model, \"lora\"):\n",
    "    model.model.lora.save_pretrained(save_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
