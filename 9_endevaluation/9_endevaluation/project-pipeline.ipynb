{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install evaluate\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install bert_score\n",
    "!pip installÂ sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For evaluation\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# --- Constants & Mappings ---\n",
    "\n",
    "PERSPECTIVES = [\"INFORMATION\", \"SUGGESTION\", \"EXPERIENCE\", \"QUESTION\", \"CAUSE\"]\n",
    "BIO_TAGS = [\"O\"] + [f\"{tag}-{p}\" for p in PERSPECTIVES for tag in [\"B\", \"I\"]]\n",
    "perspective2id = {p: i for i, p in enumerate(PERSPECTIVES)}\n",
    "id2perspective = {i: p for p, i in perspective2id.items()}\n",
    "bio2id = {t: i for i, t in enumerate(BIO_TAGS)}\n",
    "id2bio = {i: t for t, i in bio2id.items()}\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def join_answers(entry):\n",
    "    answers = entry.get(\"answers\", [])\n",
    "    if isinstance(answers, str):\n",
    "        return answers.strip()\n",
    "    if isinstance(answers, list):\n",
    "        return \" \".join(a for a in answers if isinstance(a, str)).strip()\n",
    "    return \"\"\n",
    "\n",
    "def get_reference_summaries(example):\n",
    "    \n",
    "    # Extract reference summaries from the test example.\n",
    "    # sssuming each example may contain a \"labelled_summaries\" field with keys like \"INFORMATION_SUMMARY\".\n",
    "    refs = {}\n",
    "    labelled_summaries = example.get(\"labelled_summaries\", {})\n",
    "    for perspective in PERSPECTIVES:\n",
    "        key = f\"{perspective}_SUMMARY\"\n",
    "        ref = labelled_summaries.get(key, \"\").strip()\n",
    "        if ref and ref.lower() not in [\"false\", \"true\", \"not_duplicate\", \"n/a\", \"duplicate\"]:\n",
    "            refs[perspective] = ref\n",
    "    return refs\n",
    "\n",
    "\n",
    "\n",
    "# --- Classifier Model ---\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class DualHeadClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"roberta-base\", num_perspectives=5, num_span_tags=len(BIO_TAGS)):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_perspectives)\n",
    "        self.tagger = nn.Linear(hidden_size, num_span_tags)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        cls_token = last_hidden[:, 0, :]\n",
    "        cls_logits = self.classifier(self.dropout(cls_token))\n",
    "        tag_logits = self.tagger(self.dropout(last_hidden))\n",
    "        return cls_logits, tag_logits\n",
    "\n",
    "\n",
    "\n",
    "# --- Generator Function ---\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "def generate_summary_for_perspective(input_text, perspective, generator_model, generator_tokenizer, device):\n",
    "    prompt = (\n",
    "        f\"Generate a {perspective} summary:\\n\"\n",
    "        f\"{input_text}\\n\"\n",
    "        f\"Provide a clear and structured {perspective.lower()} summary.\"\n",
    "    )\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    output_ids = generator_model.generate(input_ids=input_ids, max_length=150, num_beams=5)\n",
    "    summary = generator_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary.strip()\n",
    "\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "\n",
    "def evaluate_predictions(pipeline_results):\n",
    "    # load evaluation metrics\n",
    "    bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "    bertscore_metric = evaluate.load(\"bertscore\")\n",
    "   \n",
    "    # initialize results container per perspective.\n",
    "    eval_results = {p: {\"references\": [], \"predictions\": []} for p in PERSPECTIVES}\n",
    "   \n",
    "    for item in pipeline_results:\n",
    "        # pipeline_results items include a \"reference_summaries\" field (if available)\n",
    "        ref_summaries = item.get(\"reference_summaries\", {})\n",
    "        pred_summaries = item.get(\"generated_summaries\", {})\n",
    "        for perspective in PERSPECTIVES:\n",
    "            if perspective in ref_summaries and perspective in pred_summaries:\n",
    "                eval_results[perspective][\"references\"].append(ref_summaries[perspective])\n",
    "                eval_results[perspective][\"predictions\"].append(pred_summaries[perspective])\n",
    "   \n",
    "    # compute BLEU and BERTScore for each perspective\n",
    "    for perspective in PERSPECTIVES:\n",
    "        refs = eval_results[perspective][\"references\"]\n",
    "        preds = eval_results[perspective][\"predictions\"]\n",
    "        if refs and preds:\n",
    "            bleu_score = bleu_metric.compute(\n",
    "                predictions=preds,\n",
    "                references=[[ref] for ref in refs]\n",
    "            )[\"score\"]\n",
    "            bert_result = bertscore_metric.compute(\n",
    "                predictions=preds,\n",
    "                references=refs,\n",
    "                lang=\"en\"\n",
    "            )\n",
    "            bert_avg = np.mean(bert_result[\"f1\"])\n",
    "            print(\"{} - BLEU: {:.8f}, BERTScore: {:.4f}\".format(perspective, bleu_score, bert_avg))\n",
    "        else:\n",
    "            print(\"{} - Not enough data for evaluation.\".format(perspective))\n",
    "\n",
    "\n",
    "    \n",
    "# ---------------------------\n",
    "# --- Main Pipeline ---\n",
    "# ---------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- classifier model and tokenizer loading----------\n",
    "    \n",
    "classifier_model_path = \"/kaggle/input/dual-classifier-model/pytorch/default/1/dual_classifier_final.pt\"\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "classifier_model = DualHeadClassifier(model_name=\"roberta-base\",\n",
    "                                      num_perspectives=len(PERSPECTIVES),\n",
    "                                      num_span_tags=len(BIO_TAGS))\n",
    "classifier_state = torch.load(classifier_model_path, map_location=device)\n",
    "classifier_model.load_state_dict(classifier_state)\n",
    "classifier_model.to(device)\n",
    "classifier_model.eval()\n",
    "\n",
    "\n",
    "# ---------- generator model and tokenizer loading ----------\n",
    "\n",
    "generator_model_path = \"/kaggle/input/bard-lora-hard-2/pytorch/default/1\"\n",
    "generator_tokenizer = BartTokenizer.from_pretrained(generator_model_path)\n",
    "generator_model = BartForConditionalGeneration.from_pretrained(generator_model_path)\n",
    "generator_model.to(device)\n",
    "generator_model.eval()\n",
    "\n",
    "test_data_path = \"/kaggle/input/nlp-project/test_project.json\"\n",
    "test_data = load_json(test_data_path)\n",
    "# test_data = test_data[:int(0.01 * len(test_data))]\n",
    "\n",
    "pipeline_results = []\n",
    "\n",
    "for item in tqdm(test_data, desc=\"Processing Test Examples\"):\n",
    "    question = item.get(\"question\", \"\").strip()\n",
    "    answer = join_answers(item)\n",
    "\n",
    "    # reference summaries\n",
    "    reference_summaries = get_reference_summaries(item)\n",
    "\n",
    "    if not question or not answer:\n",
    "        continue\n",
    "\n",
    "    # combine text input as in classifier prediction\n",
    "    text = f\"Question: {question} Answer: {answer}\"\n",
    "\n",
    "    # ---------- classifier prediction ----------\n",
    "    \n",
    "    encoding = classifier_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cls_logits, _ = classifier_model(input_ids, attention_mask)\n",
    "        # apply sigmoid to get probabilities for multi-label classification\n",
    "        pred_probs = torch.sigmoid(cls_logits).squeeze(0).cpu().numpy()\n",
    "\n",
    "    # select predicted perspectives with probability > 0.5\n",
    "    predicted_perspectives = [id2perspective[i] for i, prob in enumerate(pred_probs) if prob > 0.5]\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------- generator summary for each predicted perspective ----------\n",
    "    \n",
    "    generated_summaries = {}\n",
    "    if predicted_perspectives:\n",
    "        for perspective in predicted_perspectives:\n",
    "            summary = generate_summary_for_perspective(text, perspective, generator_model, generator_tokenizer, device)\n",
    "            generated_summaries[perspective] = summary\n",
    "    else:\n",
    "        # in case no perspective exceeds threshold, generate summaries for all\n",
    "        for perspective in PERSPECTIVES:\n",
    "            summary = generate_summary_for_perspective(text, perspective, generator_model, generator_tokenizer, device)\n",
    "            generated_summaries[perspective] = summary\n",
    "\n",
    "    pipeline_results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"predicted_perspectives\": predicted_perspectives,\n",
    "        \"generated_summaries\": generated_summaries,\n",
    "        \"reference_summaries\": reference_summaries\n",
    "    })\n",
    "\n",
    "\n",
    "# Saving pipeline results to file \n",
    "output_file = \"pipeline_2_test_predictions.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(pipeline_results, f, indent=2)\n",
    "print(f\"Pipeline complete. Results saved to {output_file}\")\n",
    "\n",
    "\n",
    "# ---------- Evaluatations ----------\n",
    "\n",
    "evaluate_predictions(pipeline_results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
